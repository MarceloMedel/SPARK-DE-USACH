{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases - Computación distribuida con Apache Spark.\n",
    "\n",
    "## Objetivos\n",
    "- Instalación de librerías necesarias \n",
    "- SparkSession\n",
    "- Creación y manipulación de DataFrames\n",
    "- Operaciones básicas de DataFrames\n",
    "- Selección de columnas y filtros\n",
    "- Carga de datos a un Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de librerías necesarias\n",
    "\n",
    "Si utilizas Conda para administrar ambientes de desarrollo la mejor vía para asegurar que funcionen correctamente las librerías es instalando directo desde Conda. Adicionalmente se instalan todas las librerías que necesita Spark.\n",
    "- `conda create -n pyspark-DE`\n",
    "- `conda activate pyspark-DE`\n",
    "- `conda install -c conda-forge pyspark python=3.10`\n",
    "\n",
    "De igual forma se puede instalar a través de PIP, pero sin asegurar funcionamiento correcto ni la instalación de dependencias. Adicionalmente será necesario tener instalado Pandas.\n",
    "- `conda create -n pyspark-DE python=3.10`\n",
    "- `conda activate pyspark-DE`\n",
    "- `pip install pyspark`\n",
    "- `pip install pandas`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "\n",
    "SparkSession es una clase en PySpark que existe desde la versión 2.0 (2016) que simplifica la forma de trabajar con Spark, tanto en las configuraciones como en la manipulación de datos estructurados. \n",
    "\n",
    "#### Funcionalidades principales de SparkSession:\n",
    "\n",
    "1. **Configura Spark**: Para profundizar en las configuraciones posibles de Spark, visite https://spark.apache.org/docs/latest/configuration.html.\n",
    "    - `SparkSession.builder.appName(\"some name\").**config(\"some config key,value\")**.getOrCreate()`\n",
    "    - `spark.conf.get(\"some config key\")`\n",
    "2. **Crear DataFrames**: permite leer y escribir (Input/Outpu) diversas fuentes de datos y crear DataFrames para la manipulación de datos.\n",
    "    - `spark.createDataFrame(data [, schema])`\n",
    "    - `spark.read.json(\"path to some json\")`\n",
    "3. **Ejecutar SQL**: facilita la ejecución de consultas en SQL sobre los DataFrames.\n",
    "    - `spark.sql(\"query to some view\")`\n",
    "4. **Gestiona contexto de Spark**: facilita la configuración y acceso a diferentes componentes y funcionalidades de Spark\n",
    "    - `spark.sql.shuffle.partitions`\n",
    "    - `spark.executor.memory`\n",
    "    - `spark.catalog.listTables()`\n",
    "    - `spark.catalog.listColumns(\"someTable\")`\n",
    "    - `spark.udf.register(\"someName\", someUdf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle Partitions: 50\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una SparkSession con configuraciones personalizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConfiguracionEjemplo\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Obtener una configuración específica\n",
    "shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Shuffle Partitions: {shuffle_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application\n",
    "# config(\"spark.driver.memory\", \"1g\")\n",
    "# spark.app.name\n",
    "# spark.driver.cores, 1\n",
    "# spark.executor.memory, \"1g\" \n",
    "\n",
    "## Runtime Environment\n",
    "\n",
    "## Shuffle Behavior\n",
    "\n",
    "## Spark UI\n",
    "# spark.eventLog.enabled, false\n",
    "# spark.eventLog.buffer.kb, 100k\n",
    "# spark.ui.enabled, true\n",
    "# spark.ui.port, 4040\n",
    "\n",
    "## Compression and Serialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHATGPT\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una SparkSession con configuraciones personalizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConfiguracionEjemplo\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Mostrar las configuraciones actuales\n",
    "spark.conf.getAll()\n",
    "\n",
    "\n",
    "# Crear una tabla temporal a partir de un DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Listar las tablas del catálogo\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# Describir la estructura de una tabla\n",
    "spark.catalog.listColumns(\"people\")\n",
    "\n",
    "\n",
    "### UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Definir una función de Python\n",
    "def increment_by_one(x):\n",
    "    return x + 1\n",
    "\n",
    "# Registrar la función como UDF\n",
    "increment_udf = udf(increment_by_one, IntegerType())\n",
    "spark.udf.register(\"increment\", increment_udf)\n",
    "\n",
    "# Usar la UDF en una consulta SQL\n",
    "spark.sql(\"SELECT Name, increment(Age) as AgePlusOne FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/09 08:30:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.26:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114f76440>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.active()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame en Spark\n",
    "\n",
    "Un DataFrame es una estructura de datos bidimensional similar a cualquier tabla en una base de datos estructurada. Algunas de las características que tiene un DataFrame en PySpark son:\n",
    "- **Distribuido**: Los datos están distribuidos en un clúster de nodos, lo que permite el procesamiento paralelo.\n",
    "- **Inmutable**: Cada transformación produce un nuevo DataFrame.\n",
    "- **SQL**: Permite operaciones tipo SQL y ofrece una interfaz similar a pandas pero a gran escala.\n",
    "- **Conexiones diversas**: Puede leer datos de múltiples fuentes como JSON, CSV, Parquet, JDBC y más.\n",
    "- **Optimización Automática**: Utiliza Catalyst Optimizer para optimizar automáticamente las consultas.\n",
    "\n",
    "\n",
    "### Comparación entre PySpark DataFrame y Pandas DataFrame\n",
    "\n",
    "\n",
    "|   |Pyspark   |Pandas|\n",
    "|----------|---------|----------|\n",
    "|**Escalabilidad**| Escala horizontalmente, puede manejar terabytes o petabytes de datos distribuidos en múltiples nodos de un clúster.|Diseñado para datos que caben en memoria, generalmente utilizado para análisis de datos pequeños a medianos.|\n",
    "|**Performance**|Optimizado para el procesamiento paralelo y distribuido. Puede manejar tareas complejas y pesadas con eficiencia.|Muy rápido para operaciones en memoria y datos de tamaño moderado, pero no es adecuado para grandes volúmenes de datos debido a las limitaciones de memoria.|\n",
    "|**Optimización**|Utiliza Catalyst Optimizer para optimizar automáticamente las consultas.|No tiene optimización automática, el rendimiento depende del diseño del código del usuario.|\n",
    "|**Uso**|Ideal para grandes volúmenes de datos y procesamiento de big data en un entorno distribuido.| Adecuado para análisis de datos exploratorios, transformación de datos y preparación de datos en el entorno local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------+-------+----+-----+\n",
      "|active|             mail|nacimiento| nombre|   x|    y|\n",
      "+------+-----------------+----------+-------+----+-----+\n",
      "|  true|marcelo@correo.cl|1987-10-07|Marcelo|3232|24.36|\n",
      "| false|   juan@correo.cl|1990-05-15|   Juan|5226|26.26|\n",
      "|  true| andrea@correo.cl|1995-03-20| Andrea|2258|25.75|\n",
      "+------+-----------------+----------+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"nombre\":\"Marcelo\",\n",
    "        \"nacimiento\":date(1987, 10, 7),\n",
    "        \"mail\":\"marcelo@correo.cl\",\n",
    "        \"x\":3232,\n",
    "        \"y\":24.36,\n",
    "        \"active\":True\n",
    "    },\n",
    "    {\n",
    "        \"nombre\":\"Juan\",\n",
    "        \"nacimiento\":date(1990, 5, 15),\n",
    "        \"mail\":\"juan@correo.cl\",\n",
    "        \"x\":5226,\n",
    "        \"y\":26.26,\n",
    "        \"active\":False\n",
    "    },\n",
    "    {\n",
    "        \"nombre\":\"Andrea\",\n",
    "        \"nacimiento\":date(1995, 3, 20),\n",
    "        \"mail\":\"andrea@correo.cl\",\n",
    "        \"x\":2258,\n",
    "        \"y\":25.75,\n",
    "        \"active\":True\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----+-----+------+\n",
      "| nombre|nacimiento|             mail|   x|    y|active|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|3232|24.36|  true|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|5226|26.26| false|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|2258|25.75|  true|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    Row(nombre=\"Marcelo\", nacimiento=date(1987, 10, 7), mail=\"marcelo@correo.cl\", x=3232, y=24.36, active=True),\n",
    "    Row(nombre=\"Juan\", nacimiento=date(1990, 5, 15), mail=\"juan@correo.cl\", x=5226, y=26.26, active=False),\n",
    "    Row(nombre=\"Andrea\", nacimiento=date(1995, 3, 20), mail=\"andrea@correo.cl\", x=2258, y=25.75, active=True),\n",
    "])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- active: boolean (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+---+---+------+\n",
      "|nombre|nacimiento|mail|  x|  y|active|\n",
      "+------+----------+----+---+---+------+\n",
      "+------+----------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, FloatType, BooleanType\n",
    "\n",
    "cols=[StructField('nombre', StringType(),False),\n",
    "      StructField('nacimiento', DateType(),True),\n",
    "      StructField('mail', StringType(),True),\n",
    "      StructField('x', IntegerType(),False),\n",
    "      StructField('y', FloatType(),True),\n",
    "      StructField('active', BooleanType(),False),\n",
    "      ]\n",
    "\n",
    "schema = StructType(cols)\n",
    "\n",
    "df3 = spark.createDataFrame([], schema)\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- x: integer (nullable = false)\n",
      " |-- y: float (nullable = true)\n",
      " |-- active: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----+-----+------+\n",
      "| nombre|nacimiento|             mail|   x|    y|active|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|3232|24.36|  true|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|5226|26.26| false|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|2258|25.75|  true|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"nombre\":\"Marcelo\",\n",
    "        \"nacimiento\":date(1987, 10, 7),\n",
    "        \"mail\":\"marcelo@correo.cl\",\n",
    "        \"x\":3232,\n",
    "        \"y\":24.36,\n",
    "        \"active\":True\n",
    "    },\n",
    "    {\n",
    "        \"nombre\":\"Juan\",\n",
    "        \"nacimiento\":date(1990, 5, 15),\n",
    "        \"mail\":\"juan@correo.cl\",\n",
    "        \"x\":5226,\n",
    "        \"y\":26.26,\n",
    "        \"active\":False\n",
    "    },\n",
    "    {\n",
    "        \"nombre\":\"Andrea\",\n",
    "        \"nacimiento\":date(1995, 3, 20),\n",
    "        \"mail\":\"andrea@correo.cl\",\n",
    "        \"x\":2258,\n",
    "        \"y\":25.75,\n",
    "        \"active\":True\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "df3 = spark.createDataFrame(data, schema)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>nacimiento</th>\n",
       "      <th>mail</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marcelo</td>\n",
       "      <td>1987-10-07</td>\n",
       "      <td>marcelo@correo.cl</td>\n",
       "      <td>3232</td>\n",
       "      <td>24.36</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Juan</td>\n",
       "      <td>1990-05-15</td>\n",
       "      <td>juan@correo.cl</td>\n",
       "      <td>5226</td>\n",
       "      <td>26.26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrea</td>\n",
       "      <td>1995-03-20</td>\n",
       "      <td>andrea@correo.cl</td>\n",
       "      <td>2258</td>\n",
       "      <td>25.75</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nombre  nacimiento               mail     x      y  active\n",
       "0  Marcelo  1987-10-07  marcelo@correo.cl  3232  24.36    True\n",
       "1     Juan  1990-05-15     juan@correo.cl  5226  26.26   False\n",
       "2   Andrea  1995-03-20   andrea@correo.cl  2258  25.75    True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame(data)\n",
    "\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----+-----+------+\n",
      "| nombre|nacimiento|             mail|   x|    y|active|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|3232|24.36|  true|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|5226|26.26| false|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|2258|25.75|  true|\n",
      "+-------+----------+-----------------+----+-----+------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- nacimiento: date (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- x: long (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------\n",
      " nombre     | Marcelo           \n",
      " nacimiento | 1987-10-07        \n",
      " mail       | marcelo@correo.cl \n",
      " x          | 3232              \n",
      " y          | 24.36             \n",
      " active     | true              \n",
      "-RECORD 1-----------------------\n",
      " nombre     | Juan              \n",
      " nacimiento | 1990-05-15        \n",
      " mail       | juan@correo.cl    \n",
      " x          | 5226              \n",
      " y          | 26.26             \n",
      " active     | false             \n",
      "-RECORD 2-----------------------\n",
      " nombre     | Andrea            \n",
      " nacimiento | 1995-03-20        \n",
      " mail       | andrea@correo.cl  \n",
      " x          | 2258              \n",
      " y          | 25.75             \n",
      " active     | true              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pd_df)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nombre', 'nacimiento', 'mail', 'x', 'y', 'active']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre='Marcelo', nacimiento=datetime.date(1987, 10, 7), mail='marcelo@correo.cl', min_played=180),\n",
       " Row(nombre='Juan', nacimiento=datetime.date(1990, 5, 15), mail='juan@correo.cl', min_played=90),\n",
       " Row(nombre='Andrea', nacimiento=datetime.date(1995, 3, 20), mail='andrea@correo.cl', min_played=270)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>nacimiento</th>\n",
       "      <th>mail</th>\n",
       "      <th>min_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marcelo</td>\n",
       "      <td>1987-10-07</td>\n",
       "      <td>marcelo@correo.cl</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Juan</td>\n",
       "      <td>1990-05-15</td>\n",
       "      <td>juan@correo.cl</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrea</td>\n",
       "      <td>1995-03-20</td>\n",
       "      <td>andrea@correo.cl</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nombre  nacimiento               mail  min_played\n",
       "0  Marcelo  1987-10-07  marcelo@correo.cl         180\n",
       "1     Juan  1990-05-15     juan@correo.cl          90\n",
       "2   Andrea  1995-03-20   andrea@correo.cl         270"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df = df.toPandas()\n",
    "ps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'nombre'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "|Marcelo|\n",
      "|   Juan|\n",
      "| Andrea|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "|Marcelo|\n",
      "|   Juan|\n",
      "| Andrea|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('nombre').show()\n",
    "df.select(df.nombre).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----------+------------+\n",
      "| nombre|nacimiento|             mail|min_played|upper_nombre|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|       180|     MARCELO|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|        90|        JUAN|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|       270|      ANDREA|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('upper_nombre',upper(df.nombre)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----------+\n",
      "| nombre|nacimiento|             mail|min_played|\n",
      "+-------+----------+-----------------+----------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|       180|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|        90|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|       270|\n",
      "+-------+----------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----------+------------+\n",
      "| nombre|nacimiento|             mail|min_played|upper_nombre|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|       180|     MARCELO|\n",
      "|   Juan|1990-05-15|   juan@correo.cl|        90|        JUAN|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|       270|      ANDREA|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('upper_nombre',upper(df.nombre))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+----------+------------+\n",
      "| nombre|nacimiento|             mail|min_played|upper_nombre|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|       180|     MARCELO|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|       270|      ANDREA|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "| nombre|nacimiento|             mail|min_played|upper_nombre|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "|Marcelo|1987-10-07|marcelo@correo.cl|       180|     MARCELO|\n",
      "| Andrea|1995-03-20| andrea@correo.cl|       270|      ANDREA|\n",
      "+-------+----------+-----------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.min_played > 90).show()\n",
    "\n",
    "df.filter(\"min_played > 90\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.appName(\"example\").getOrCreate().stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-DE-USACH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
